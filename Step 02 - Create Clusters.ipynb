{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 02 - Create Clusters\n",
    "We need to order our learning data to clusters. When we have the clusters we can count for each test data the closest cluster. Then based on the learning data in that cluster we can make peak load predictions for our test data.\n",
    "\n",
    "The make it possible to count distance between our clusters and test data, the dimension of the test data and the vectors in the cluster must be equal. As the test data has lower dimension (it's resoultion is daily not half-hourly) first we need to reduce the resolution of the half-hourly learning data to become daily. Note: we need to keep the original resolution as well for the peak load predicitons later.\n",
    "\n",
    "After we reduced the resolution we can create clusters. We use [spectral clustering](https://towardsdatascience.com/spectral-clustering-aba2640c0d5b) to cluster it. The number of clusters needs to be set in the program, usually the less than 10 is a good chice.\n",
    "Note: the project could be improved by trying different number of clusters, finding the number with best accuracy.\n",
    "\n",
    "After clustering the reduced dimension learning data vectors. Count the average of vectors in each cluster, creating a cluster-center vector. At this point we can start using the prepared test data.\n",
    "\n",
    "For each test data vectors we count the distance between the cluster-center vectors. We group each test data vectors to the closest cluster. And based on the (high resolution version of) learning data vectors in the cluster we can give half-hourly load prediction to the low resolution learning data vectors.\n",
    "\n",
    "### Note:\n",
    "The code below use same hand typed example data. Here we do not use the real big data sets yet. (Not even the cleaned and interpolated meta reading data.) The code is only illustration how the process works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 2, 6],\n",
       " [3, 8, 2],\n",
       " [1.1, 8, 10],\n",
       " [2.0, 1, 11],\n",
       " [2.1, 5, 11],\n",
       " [11, 1, 6],\n",
       " [1, 1, 10]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#smd: smart meter data -> 9 points / record\n",
    "# \"learning data\"\n",
    "smd = [[1,3,4, 1,0,1, 2,1,3],    # (X._.X)   1   # <-- clusterd by hand, to see how well the \n",
    "       [0,1,2, 4,3,1, 1,1,0],    # |_.X._|   2   #     algorithm works \n",
    "       [.1,.8,.2, 3,3,2, 3,4,3], # |_.X.X|   2\n",
    "       [.9,.1,1, 0,0,1, 4,4,3],  # <_._.X>   0\n",
    "       [.2, .9,1, 0,3,2, 3,3,5], # |_.X.X|   2\n",
    "       [3,4,4, 1,0,0, 3,1,2],    # (X._.X)   1\n",
    "       [0,0,1, 0,1,0, 4,1,5]]    # <_._.X>   0\n",
    "\n",
    "# mr: meta reading data\n",
    "# \"test data\"\n",
    "mr = [[9,1,3],                    # (X._.x)    1\n",
    "      [0.8,7,15],                 # |_.X.X|    2   <-- pay attention to this (between type 2 and type 0)\n",
    "      [3,3,12]]                   # <_._.X>    0  \n",
    "\n",
    "smd2 = []\n",
    "\n",
    "#  reducing the resolution to mach with the test data's resolution \n",
    "for vec in smd:\n",
    "    smd2.append([sum(vec[:3]),sum(vec[3:6]), sum(vec[6:])]) # needs to be automated, now splitted by \"hand\"\n",
    "    \n",
    "D = distance_matrix(smd2, smd2)\n",
    "smd2  # small resolution version of smart reading (\"learning-\") data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\spectral.py:442: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n"
     ]
    }
   ],
   "source": [
    "N = 3 # number of clusters\n",
    "clustering = SpectralClustering(n_clusters=N,\n",
    "         assign_labels=\"discretize\",\n",
    "         random_state=0).fit(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 0, 1, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smLabels = clustering.labels_ # with same indexing as input we find cluster id-s in this\n",
    "smLabels   # smart meter labels, the clustering algorithm groups vectors to the same groups we did \"by hand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = [zeros(len(smd2[0]))] * N \n",
    "# (number of clusters) times null-vectors\n",
    "\n",
    "for i in range(len(smd2)): # sum of vectors in each cluster\n",
    "    vec = smd2[i]\n",
    "    smLabel = smLabels[i] # actual cluster ID\n",
    "    centers[smLabel] = [sum(x) for x in zip(vec, centers[smLabel])]\n",
    "    \n",
    "for smLabel in range(N): # divide it by the number of vectors in each cluster\n",
    "    centers[smLabel] = [x / smLabels.tolist().count(smLabel) for x in centers[smLabel]]\n",
    "\n",
    "D2 = distance_matrix(mr, centers)\n",
    "mrLabels = [] # gives the id of the closest cluster for each \n",
    "            # meta reading data point\n",
    "for row in D2:\n",
    "    mrLabels.append(argmin(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we know the closest clusters for each meta reading\n",
    "# next step to fill the peak loads\n",
    "\n",
    "prediction = [[0. for i in range(len(smd[0]))] for j in range(N)]\n",
    "'''We are going to make one prediction to each cluster.\n",
    "The meta reading data vectors will get the prediction\n",
    "of the closest cluster.'''        \n",
    "\n",
    "\n",
    "#   Summarize vectors in each cluster:                 \n",
    "for i in range(len(smd[0])):  # assume that all vectors in smd has the same length (== smd[0])\n",
    "    for k in range(len(smd)):\n",
    "        prediction[smLabels[k]][i] += smd[k][i]\n",
    "\n",
    "#   Devide the sum by the number of vectors in each cluster\n",
    "for i in range(N):\n",
    "    norm = smLabels.tolist().count(i) \n",
    "    prediction[i] = [j / norm for j in prediction[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4.0, 7.0, 8.0, 2.0, 0.0, 1.0, 5.0, 2.0, 5.0],\n",
       " [0.30000000000000004, 2.7, 3.2, 7.0, 9.0, 5.0, 7.0, 8.0, 8.0],\n",
       " [0.9, 0.1, 2.0, 0.0, 1.0, 1.0, 8.0, 5.0, 8.0]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the half-hourly load prediction for each meta reading vector:\n",
    "mr2 = mr\n",
    "\n",
    "for i in range(len(mr2)):\n",
    "    mr2[i] = prediction[mrLabels[i]]\n",
    "\n",
    "mr2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
